{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10e92c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_X 확인 \n",
      "           age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
      "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
      "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
      "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
      "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
      "\n",
      "           s4        s5        s6  \n",
      "0   -0.002592  0.019908 -0.017646  \n",
      "1   -0.039493 -0.068330 -0.092204  \n",
      "2   -0.002592  0.002864 -0.025930  \n",
      "3    0.034309  0.022692 -0.009362  \n",
      "4   -0.002592 -0.031991 -0.046641  \n",
      "..        ...       ...       ...  \n",
      "437 -0.002592  0.031193  0.007207  \n",
      "438  0.034309 -0.018118  0.044485  \n",
      "439 -0.011080 -0.046879  0.015491  \n",
      "440  0.026560  0.044528 -0.025930  \n",
      "441 -0.039493 -0.004220  0.003064  \n",
      "\n",
      "[442 rows x 10 columns]\n",
      "df_y 확인 \n",
      " 0      151.0\n",
      "1       75.0\n",
      "2      141.0\n",
      "3      206.0\n",
      "4      135.0\n",
      "       ...  \n",
      "437    178.0\n",
      "438    104.0\n",
      "439    132.0\n",
      "440    220.0\n",
      "441     57.0\n",
      "Name: target, Length: 442, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# (1)데이터 가져오기\n",
    "# sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# diabetes의 data를 df_X에, target을 df_y에 저장해주세요.\n",
    "df_X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df_y = pd.Series(diabetes.target, name='target')\n",
    "\n",
    "print(\"df_X 확인 \\n\", df_X)\n",
    "print(\"df_y 확인 \\n\", df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cde233d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "# (2)모델에 입력할 데이터 X 준비하기\n",
    "# df_X에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "X = df_X.values\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c195198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "# (3) 모델에 예측할 데이터 y 준비하기\n",
    "# df_y에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "y = df_y.values\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1868a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n",
      "\n",
      "\n",
      "X_train \n",
      " [[ 0.07076875  0.05068012  0.01211685 ...  0.03430886  0.02736771\n",
      "  -0.0010777 ]\n",
      " [-0.00914709  0.05068012 -0.01806189 ...  0.07120998  0.00027149\n",
      "   0.01963284]\n",
      " [ 0.00538306 -0.04464164  0.04984027 ... -0.00259226  0.01703713\n",
      "  -0.01350402]\n",
      " ...\n",
      " [ 0.03081083 -0.04464164 -0.02021751 ... -0.03949338 -0.01090444\n",
      "  -0.0010777 ]\n",
      " [-0.01277963 -0.04464164 -0.02345095 ... -0.00259226 -0.03845911\n",
      "  -0.03835666]\n",
      " [-0.09269548 -0.04464164  0.02828403 ... -0.03949338 -0.00514531\n",
      "  -0.0010777 ]]\n",
      "\n",
      "\n",
      "X_test \n",
      "\n",
      " [[ 0.04534098 -0.04464164 -0.00620595 -0.01599922  0.1250187   0.1251981\n",
      "   0.019187    0.03430886  0.03243323 -0.0052198 ]\n",
      " [ 0.09256398 -0.04464164  0.03690653  0.02187235 -0.02496016 -0.01665815\n",
      "   0.00077881 -0.03949338 -0.02251217 -0.02178823]\n",
      " [ 0.06350368  0.05068012 -0.00405033 -0.01255635  0.10300346  0.04878988\n",
      "   0.05600338 -0.00259226  0.08449528 -0.01764613]\n",
      " [ 0.09619652 -0.04464164  0.0519959   0.07925353  0.05484511  0.03657709\n",
      "  -0.07653559  0.14132211  0.09864637  0.06105391]\n",
      " [ 0.01264814  0.05068012 -0.02021751 -0.00222774  0.03833367  0.05317395\n",
      "  -0.00658447  0.03430886 -0.00514531 -0.00936191]\n",
      " [ 0.0090156  -0.04464164 -0.02452876 -0.02632783  0.0988756   0.0941964\n",
      "   0.07072993 -0.00259226 -0.02139368  0.00720652]\n",
      " [-0.00914709  0.05068012  0.17055523  0.01498661  0.03007796  0.03375875\n",
      "  -0.02131102  0.03430886  0.03365681  0.03205916]\n",
      " [-0.02367725  0.05068012  0.04552903  0.02187235  0.10988322  0.08887288\n",
      "   0.00077881  0.03430886  0.07419254  0.06105391]\n",
      " [-0.09269548  0.05068012 -0.0902753  -0.05731367 -0.02496016 -0.03043668\n",
      "  -0.00658447 -0.00259226  0.02405258  0.00306441]\n",
      " [-0.06000263  0.05068012  0.01535029 -0.01944209  0.03695772  0.04816358\n",
      "   0.019187   -0.00259226 -0.03075121 -0.0010777 ]\n",
      " [-0.04183994 -0.04464164 -0.03315126 -0.02288496  0.04658939  0.04158746\n",
      "   0.05600338 -0.02473293 -0.02595242 -0.03835666]\n",
      " [ 0.00538306 -0.04464164 -0.05794093 -0.02288496 -0.0676147  -0.06832765\n",
      "  -0.05444576 -0.00259226  0.04289569 -0.08391984]\n",
      " [-0.08906294 -0.04464164 -0.06117437 -0.02632783 -0.05523112 -0.05454912\n",
      "   0.04127682 -0.0763945  -0.09393565 -0.05492509]\n",
      " [ 0.01991321  0.05068012  0.01427248  0.0631868   0.01494247  0.02029337\n",
      "  -0.04708248  0.03430886  0.04666077  0.09004865]\n",
      " [-0.01277963  0.05068012 -0.05578531 -0.00222774 -0.02771206 -0.02918409\n",
      "   0.019187   -0.03949338 -0.0170521   0.04448548]\n",
      " [-0.03457486  0.05068012  0.00564998 -0.00567061 -0.07311851 -0.06269098\n",
      "  -0.00658447 -0.03949338 -0.04542096  0.03205916]\n",
      " [ 0.04170844  0.05068012  0.07139652  0.00810087  0.03833367  0.01590929\n",
      "  -0.01762938  0.03430886  0.07341008  0.08590655]\n",
      " [ 0.06350368  0.05068012  0.08864151  0.07007254  0.02044629  0.03751653\n",
      "  -0.05076412  0.07120998  0.02930041  0.07348023]\n",
      " [-0.03094232  0.05068012  0.02828403  0.07007254 -0.12678067 -0.10684491\n",
      "  -0.05444576 -0.04798064 -0.03075121  0.01549073]\n",
      " [ 0.04897352  0.05068012  0.05846277  0.07007254  0.01356652  0.02060651\n",
      "  -0.02131102  0.03430886  0.02200405  0.02791705]\n",
      " [-0.07090025 -0.04464164  0.03906215 -0.03321358 -0.01257658 -0.03450761\n",
      "  -0.02499266 -0.00259226  0.06773633 -0.01350402]\n",
      " [ 0.04897352 -0.04464164 -0.04285156 -0.0538708   0.04521344  0.05004247\n",
      "   0.03391355 -0.00259226 -0.02595242 -0.0632093 ]\n",
      " [-0.04183994 -0.04464164 -0.04931844 -0.03665645 -0.00707277 -0.02260797\n",
      "   0.08545648 -0.03949338 -0.06648815  0.00720652]\n",
      " [ 0.05987114  0.05068012  0.02289497  0.04941532  0.01631843  0.01183836\n",
      "  -0.01394774 -0.00259226  0.03953988  0.01963284]\n",
      " [-0.06000263 -0.04464164  0.04445121 -0.01944209 -0.00982468 -0.00757685\n",
      "   0.02286863 -0.03949338 -0.02712865 -0.00936191]\n",
      " [ 0.04170844  0.05068012  0.01427248  0.04252958 -0.03046397 -0.00131388\n",
      "  -0.04340085 -0.00259226 -0.03324879  0.01549073]\n",
      " [ 0.03081083  0.05068012  0.05954058  0.05630106 -0.02220825  0.00119131\n",
      "  -0.03235593 -0.00259226 -0.02479119 -0.01764613]\n",
      " [ 0.03807591  0.05068012 -0.01806189  0.06662967 -0.05110326 -0.01665815\n",
      "  -0.07653559  0.03430886 -0.01190068 -0.01350402]\n",
      " [-0.05637009 -0.04464164 -0.07410811 -0.05042793 -0.02496016 -0.04703355\n",
      "   0.09281975 -0.0763945  -0.0611766  -0.04664087]\n",
      " [ 0.01628068  0.05068012 -0.02129532 -0.00911348  0.03420581  0.04785043\n",
      "   0.00077881 -0.00259226 -0.01290794  0.02377494]\n",
      " [ 0.02717829 -0.04464164  0.09295276 -0.05272318  0.00806271  0.03970857\n",
      "  -0.02867429  0.02102446 -0.04836172  0.01963284]\n",
      " [-0.10359309 -0.04464164 -0.0374625  -0.02632783  0.0025589   0.01998022\n",
      "   0.01182372 -0.00259226 -0.06832974 -0.02593034]\n",
      " [-0.00188202  0.05068012  0.01427248 -0.07452802  0.0025589   0.00620169\n",
      "  -0.01394774 -0.00259226  0.01919903  0.00306441]\n",
      " [ 0.02717829  0.05068012 -0.00620595  0.0287581  -0.01670444 -0.00162703\n",
      "  -0.0581274   0.03430886  0.02930041  0.03205916]\n",
      " [ 0.09619652 -0.04464164  0.04013997 -0.05731367  0.04521344  0.06068952\n",
      "  -0.02131102  0.03615391  0.01255315  0.02377494]\n",
      " [-0.07816532  0.05068012  0.07786339  0.05285819  0.07823631  0.0644473\n",
      "   0.02655027 -0.00259226  0.04067226 -0.00936191]\n",
      " [-0.00914709  0.05068012 -0.03099563 -0.02632783 -0.01120063 -0.00100073\n",
      "  -0.02131102 -0.00259226  0.00620932  0.02791705]\n",
      " [-0.00188202  0.05068012 -0.03315126 -0.01829447  0.03145391  0.04284006\n",
      "  -0.01394774  0.01991742  0.01022564  0.02791705]\n",
      " [ 0.06713621  0.05068012 -0.03099563  0.004658    0.02457414  0.03563764\n",
      "  -0.02867429  0.03430886  0.02337484  0.08176444]\n",
      " [ 0.01991321 -0.04464164 -0.05794093 -0.05731367 -0.00156896 -0.01258722\n",
      "   0.07441156 -0.03949338 -0.0611766  -0.07563562]\n",
      " [-0.00188202 -0.04464164 -0.06979687 -0.01255635 -0.00019301 -0.00914259\n",
      "   0.07072993 -0.03949338 -0.06291295  0.04034337]\n",
      " [ 0.06713621  0.05068012 -0.02991782  0.05744869 -0.00019301 -0.01571871\n",
      "   0.07441156 -0.05056372 -0.03845911  0.00720652]\n",
      " [ 0.04170844  0.05068012 -0.02237314  0.0287581  -0.06623874 -0.04515466\n",
      "  -0.06180903 -0.00259226  0.00286377 -0.05492509]\n",
      " [ 0.11072668  0.05068012 -0.03315126 -0.02288496 -0.00432087  0.02029337\n",
      "  -0.06180903  0.07120998  0.01556684  0.04448548]\n",
      " [ 0.04170844 -0.04464164 -0.04500719  0.03449621  0.04383748 -0.01571871\n",
      "   0.03759519 -0.01440062  0.08989869  0.00720652]\n",
      " [-0.01641217 -0.04464164 -0.03530688 -0.02632783  0.03282986  0.01716188\n",
      "   0.10018303 -0.03949338 -0.07020931 -0.07977773]\n",
      " [-0.09632802 -0.04464164 -0.03638469 -0.07452802 -0.03871969 -0.02761835\n",
      "   0.01550536 -0.03949338 -0.07408887 -0.0010777 ]\n",
      " [ 0.03081083 -0.04464164 -0.05039625 -0.00222774 -0.0442235  -0.08993489\n",
      "   0.11859122 -0.0763945  -0.01811827  0.00306441]\n",
      " [-0.02004471 -0.04464164 -0.08488624 -0.02632783 -0.03596778 -0.03419447\n",
      "   0.04127682 -0.05167075 -0.08238148 -0.04664087]\n",
      " [-0.06000263 -0.04464164  0.00133873 -0.02977071 -0.00707277 -0.02166853\n",
      "   0.01182372 -0.00259226  0.03181522 -0.05492509]\n",
      " [ 0.00538306 -0.04464164  0.05846277 -0.04354219 -0.07311851 -0.07239858\n",
      "   0.019187   -0.0763945  -0.05140054 -0.02593034]\n",
      " [-0.09632802 -0.04464164 -0.06979687 -0.06764228 -0.01945635 -0.01070833\n",
      "   0.01550536 -0.03949338 -0.04687948 -0.07977773]\n",
      " [ 0.02717829  0.05068012  0.01750591 -0.03321358 -0.00707277  0.04597154\n",
      "  -0.06549067  0.07120998 -0.09643322 -0.05906719]\n",
      " [ 0.01991321 -0.04464164 -0.04069594 -0.01599922 -0.00844872 -0.0175976\n",
      "   0.05232174 -0.03949338 -0.03075121  0.00306441]\n",
      " [-0.05273755  0.05068012 -0.01806189  0.08040116  0.08924393  0.10766179\n",
      "  -0.03971921  0.1081111   0.03605579 -0.04249877]\n",
      " [-0.02730979 -0.04464164  0.06492964 -0.00222774 -0.02496016 -0.01728445\n",
      "   0.02286863 -0.03949338 -0.0611766  -0.0632093 ]\n",
      " [-0.02367725 -0.04464164 -0.046085   -0.03321358  0.03282986  0.03626394\n",
      "   0.03759519 -0.00259226 -0.03324879  0.01134862]\n",
      " [ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      "  -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      " [-0.02730979 -0.04464164 -0.01806189 -0.04009932 -0.00294491 -0.01133463\n",
      "   0.03759519 -0.03949338 -0.00894402 -0.05492509]\n",
      " [-0.0382074  -0.04464164 -0.0547075  -0.0779709  -0.03321588 -0.08649026\n",
      "   0.14068104 -0.0763945  -0.01919705 -0.0052198 ]\n",
      " [-0.02367725 -0.04464164  0.03043966 -0.00567061  0.08236416  0.09200436\n",
      "  -0.01762938  0.07120998  0.03304707  0.00306441]\n",
      " [-0.04183994 -0.04464164  0.04121778 -0.02632783 -0.03183992 -0.03043668\n",
      "  -0.03603757  0.00294291  0.03365681 -0.01764613]\n",
      " [-0.06000263  0.05068012  0.05415152 -0.01944209 -0.04972731 -0.04891244\n",
      "   0.02286863 -0.03949338 -0.0439854  -0.0052198 ]\n",
      " [ 0.01628068  0.05068012 -0.04500719  0.0631868   0.01081462 -0.00037443\n",
      "   0.06336665 -0.03949338 -0.03075121  0.03620126]\n",
      " [-0.04183994 -0.04464164 -0.06548562 -0.04009932 -0.00569682  0.01434355\n",
      "  -0.04340085  0.03430886  0.00702686 -0.01350402]\n",
      " [ 0.07440129 -0.04464164  0.01858372  0.0631868   0.06172487  0.04284006\n",
      "   0.00814208 -0.00259226  0.05803913 -0.05906719]\n",
      " [-0.05273755  0.05068012 -0.01159501  0.05630106  0.05622106  0.07290231\n",
      "  -0.03971921  0.07120998  0.03056649 -0.0052198 ]\n",
      " [ 0.05260606  0.05068012 -0.02452876  0.05630106 -0.00707277 -0.00507166\n",
      "  -0.02131102 -0.00259226  0.02671426 -0.03835666]\n",
      " [-0.10722563 -0.04464164 -0.03422907 -0.06764228 -0.06348684 -0.07051969\n",
      "   0.00814208 -0.03949338 -0.00060925 -0.07977773]\n",
      " [-0.06726771  0.05068012 -0.01267283 -0.04009932 -0.01532849  0.00463594\n",
      "  -0.0581274   0.03430886  0.01919903 -0.03421455]\n",
      " [-0.07453279  0.05068012 -0.01806189  0.00810087 -0.01945635 -0.02480001\n",
      "  -0.06549067  0.03430886  0.06731722 -0.01764613]\n",
      " [-0.00188202  0.05068012  0.03043966  0.05285819  0.03970963  0.05661859\n",
      "  -0.03971921  0.07120998  0.02539313  0.02791705]\n",
      " [ 0.05987114 -0.04464164 -0.02129532  0.0872869   0.04521344  0.03156671\n",
      "  -0.04708248  0.07120998  0.07912108  0.13561183]\n",
      " [-0.06000263  0.05068012 -0.0105172  -0.0148516  -0.04972731 -0.02354742\n",
      "  -0.0581274   0.0158583  -0.00991896 -0.03421455]\n",
      " [ 0.06713621 -0.04464164 -0.06117437 -0.04009932 -0.02633611 -0.02448686\n",
      "   0.03391355 -0.03949338 -0.05615757 -0.05906719]\n",
      " [ 0.0090156   0.05068012 -0.03961813  0.0287581   0.03833367  0.0735286\n",
      "  -0.07285395  0.1081111   0.01556684 -0.04664087]\n",
      " [-0.02730979  0.05068012  0.06061839  0.04941532  0.08511607  0.08636769\n",
      "  -0.00290283  0.03430886  0.03781448  0.04862759]\n",
      " [-0.04547248 -0.04464164  0.03906215  0.00121513  0.01631843  0.01528299\n",
      "  -0.02867429  0.02655962  0.04452837 -0.02593034]\n",
      " [ 0.04534098  0.05068012  0.01966154  0.03908671  0.02044629  0.02593004\n",
      "   0.00814208 -0.00259226 -0.00330371  0.01963284]\n",
      " [ 0.01264814 -0.04464164 -0.02021751 -0.01599922  0.01219057  0.02123281\n",
      "  -0.07653559  0.1081111   0.05988072 -0.02178823]\n",
      " [-0.0854304  -0.04464164 -0.00405033 -0.00911348 -0.00294491  0.00776743\n",
      "   0.02286863 -0.03949338 -0.0611766  -0.01350402]\n",
      " [-0.05637009 -0.04464164 -0.01159501 -0.03321358 -0.0469754  -0.04765985\n",
      "   0.00446045 -0.03949338 -0.0079794  -0.08806194]\n",
      " [-0.04910502 -0.04464164 -0.06440781 -0.10207099 -0.00294491 -0.01540556\n",
      "   0.06336665 -0.04724262 -0.03324879 -0.05492509]\n",
      " [-0.02730979 -0.04464164 -0.06009656 -0.02977071  0.04658939  0.01998022\n",
      "   0.12227286 -0.03949338 -0.05140054 -0.00936191]\n",
      " [ 0.00175052 -0.04464164 -0.06548562 -0.00567061 -0.00707277 -0.01947649\n",
      "   0.04127682 -0.03949338 -0.00330371  0.00720652]\n",
      " [ 0.01264814 -0.04464164 -0.02560657 -0.04009932 -0.03046397 -0.04515466\n",
      "   0.0780932  -0.0763945  -0.07212845  0.01134862]\n",
      " [-0.02730979 -0.04464164 -0.06332999 -0.05042793 -0.08962994 -0.10433972\n",
      "   0.05232174 -0.0763945  -0.05615757 -0.06735141]\n",
      " [-0.02367725 -0.04464164 -0.06979687 -0.06419941 -0.05935898 -0.05047819\n",
      "   0.019187   -0.03949338 -0.08913686 -0.05078298]\n",
      " [-0.06363517 -0.04464164  0.03582872 -0.02288496 -0.03046397 -0.01885019\n",
      "  -0.00658447 -0.00259226 -0.02595242 -0.05492509]]\n",
      "\n",
      "\n",
      "y_train \n",
      "\n",
      " [144. 150. 280. 125.  59.  65. 281. 277. 167.  90.  72. 178.  88. 270.\n",
      " 101. 197.  97.  53.  71. 262.  52. 102. 166. 189. 173. 220. 206.  97.\n",
      "  60.  61. 242. 121. 128. 104. 265. 132. 283. 174. 129. 257. 137.  63.\n",
      "  93. 232. 208. 261. 179. 258. 262.  51. 237.  71. 139. 268.  69. 317.\n",
      " 249. 154. 192. 116.  81. 122. 259. 191. 292.  55. 107. 210.  91. 253.\n",
      "  85. 252.  59.  78. 200.  78. 245. 175.  42. 127.  53.  94. 104. 199.\n",
      " 265. 281. 248. 257. 215. 303. 170.  59. 277. 209. 138. 198. 124.  96.\n",
      " 288. 225. 265. 101.  55. 198.  51. 252.  64. 220. 131. 212. 142. 103.\n",
      " 155. 121.  86. 111.  65. 131.  51. 128. 141.  48. 109. 178.  88.  84.\n",
      " 216. 150.  60.  96. 190.  74. 279. 182. 160. 245. 276. 174. 180. 150.\n",
      " 196. 138.  97. 246. 321. 308. 109.  69. 182. 258. 161. 178. 214.  45.\n",
      " 150. 160.  55. 197. 185. 268. 310. 123.  68.  72. 185. 144. 147. 168.\n",
      " 178. 246. 151. 127.  83. 332. 152. 109.  90.  66. 214.  85. 129.  89.\n",
      " 259. 229. 200.  77.  54.  31. 109. 206. 144. 118.  83. 242. 259.  72.\n",
      " 163. 181. 141.  71. 137. 195. 179. 102. 131.  47. 235.  77. 198.  93.\n",
      " 162. 225. 275. 183. 306.  81.  55. 146. 196. 230. 310.  40. 135. 346.\n",
      "  43. 128.  77. 235.  49.  74.  92.  84. 263. 144. 142. 341. 115. 158.\n",
      " 273.  85.  88. 220.  39.  80. 172. 217. 336.  52. 272. 115. 110. 131.\n",
      "  71. 275. 118.  25. 100. 281. 221. 248. 200. 132.  91.  67. 202.  73.\n",
      "  85. 275. 243.  66. 293. 236. 243.  87.  39. 217.  92. 296. 292. 142.\n",
      "  50.  53. 104.  75. 120. 142. 143.  99.  65. 116. 233. 164.  95.  59.\n",
      " 139. 145. 177. 185.  97.  42. 201. 241.  70.  78.  49. 103.  44. 111.\n",
      " 191.  47. 182.  58. 155. 151.  79. 104. 143. 152. 170.  75. 200. 124.\n",
      "  91.  49. 163.  53. 283. 178. 219. 200. 113. 113.  63. 114. 126. 274.\n",
      "  88. 311.  83.  71. 134. 244.  65. 173.  57.  68. 141. 270. 134. 202.\n",
      " 148.  64. 302.]\n",
      "\n",
      "\n",
      "y_test \n",
      "\n",
      " [219.  70. 202. 230. 111.  84. 242. 272.  94.  96.  94. 252.  99. 297.\n",
      " 135.  67. 295. 264. 170. 275. 310.  64. 128. 232. 129. 118. 263.  77.\n",
      "  48. 107. 140. 113.  90. 164. 180. 233.  42.  84. 172.  63.  48. 108.\n",
      " 156. 168.  90.  52. 200.  87.  90. 258. 136. 158.  69.  72. 171.  95.\n",
      "  72. 151. 168.  60. 122.  52. 187. 102. 214. 248. 181. 110. 140. 202.\n",
      " 101. 222. 281.  61.  89.  91. 186. 220. 237. 233.  68. 190.  96.  72.\n",
      " 153.  98.  37.  63. 184.]\n"
     ]
    }
   ],
   "source": [
    "# (4) train 데이터와 test 데이터로 분리하기\n",
    "# X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\n\\nX_train \\n\", X_train)\n",
    "print(\"\\n\\nX_test \\n\\n\", X_test)\n",
    "print(\"\\n\\ny_train \\n\\n\", y_train)\n",
    "print(\"\\n\\ny_test \\n\\n\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4351294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W 확인 \n",
      " [ 0.97053411  1.27540331 -1.12218408 -1.44957648  0.42503087 -0.82181386\n",
      " -1.20394222  2.34570912 -1.46471671 -1.76491746]\n",
      "b 확인 \n",
      " 1.968122753407076\n"
     ]
    }
   ],
   "source": [
    "# (5) 모델 준비하기\n",
    "# 입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.\n",
    "import numpy as np\n",
    "\n",
    "W = np.random.randn(X_train.shape[1])\n",
    "b = np.random.randn()\n",
    "\n",
    "print(\"W 확인 \\n\", W)\n",
    "print(\"b 확인 \\n\", b)\n",
    "\n",
    "# 모델 함수를 구현해주세요.\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(X_train.shape[1]):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "919262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) 손실함수 loss 정의하기\n",
    "# 손실함수를 MSE 함수로 정의해주세요.\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70a7a96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "# 기울기를 계산하는 gradient 함수를 구현해주세요.\n",
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a602d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "# 학습률, learning rate 를 설정해주세요\n",
    "# 만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요.\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "889f15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 : Loss 5394.0865\n",
      "Iteration 2000 : Loss 4888.7367\n",
      "Iteration 3000 : Loss 4512.9166\n",
      "Iteration 4000 : Loss 4229.7204\n",
      "Iteration 5000 : Loss 4013.1782\n",
      "Iteration 6000 : Loss 3844.9705\n",
      "Iteration 7000 : Loss 3712.1348\n",
      "Iteration 8000 : Loss 3605.4642\n",
      "Iteration 9000 : Loss 3518.3879\n",
      "Iteration 10000 : Loss 3446.1892\n",
      "Iteration 11000 : Loss 3385.4582\n",
      "Iteration 12000 : Loss 3333.7084\n",
      "Iteration 13000 : Loss 3289.1089\n",
      "Iteration 14000 : Loss 3250.2956\n",
      "Iteration 15000 : Loss 3216.2386\n",
      "Iteration 16000 : Loss 3186.1495\n",
      "Iteration 17000 : Loss 3159.4151\n",
      "Iteration 18000 : Loss 3135.5507\n",
      "Iteration 19000 : Loss 3114.1671\n",
      "Iteration 20000 : Loss 3094.9470\n",
      "Iteration 21000 : Loss 3077.6272\n",
      "Iteration 22000 : Loss 3061.9872\n",
      "Iteration 23000 : Loss 3047.8392\n",
      "Iteration 24000 : Loss 3035.0221\n",
      "Iteration 25000 : Loss 3023.3960\n",
      "Iteration 26000 : Loss 3012.8389\n",
      "Iteration 27000 : Loss 3003.2432\n",
      "Iteration 28000 : Loss 2994.5138\n",
      "Iteration 29000 : Loss 2986.5662\n",
      "Iteration 30000 : Loss 2979.3253\n",
      "Iteration 31000 : Loss 2972.7236\n",
      "Iteration 32000 : Loss 2966.7007\n",
      "Iteration 33000 : Loss 2961.2025\n",
      "Iteration 34000 : Loss 2956.1801\n",
      "Iteration 35000 : Loss 2951.5897\n",
      "Iteration 36000 : Loss 2947.3914\n",
      "Iteration 37000 : Loss 2943.5497\n",
      "Iteration 38000 : Loss 2940.0321\n",
      "Iteration 39000 : Loss 2936.8093\n",
      "Iteration 40000 : Loss 2933.8550\n",
      "Iteration 41000 : Loss 2931.1451\n",
      "Iteration 42000 : Loss 2928.6579\n",
      "Iteration 43000 : Loss 2926.3738\n",
      "Iteration 44000 : Loss 2924.2749\n",
      "Iteration 45000 : Loss 2922.3450\n",
      "Iteration 46000 : Loss 2920.5694\n",
      "Iteration 47000 : Loss 2918.9348\n",
      "Iteration 48000 : Loss 2917.4289\n",
      "Iteration 49000 : Loss 2916.0409\n",
      "Iteration 50000 : Loss 2914.7605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([139.5483133 , 179.52030578, 134.04133298, 291.41193598,\n",
       "       123.78723656,  92.17357677, 258.23409704, 181.33895238,\n",
       "        90.22217862, 108.63143298,  94.13938654, 168.43379636,\n",
       "        53.50669663, 206.63040068, 100.13238561, 130.66881649,\n",
       "       219.53270758, 250.78291772, 196.36682356, 218.57497401,\n",
       "       207.35002447,  88.48361667,  70.43428801, 188.95725301,\n",
       "       154.88720039, 159.35957695, 188.31587948, 180.38835506,\n",
       "        47.98988446, 108.97514644, 174.78080029,  86.36598906,\n",
       "       132.95890535, 184.5410226 , 173.83298051, 190.35863287,\n",
       "       124.41740796, 119.65426903, 147.95402494,  59.05311211,\n",
       "        71.62636914, 107.68722902, 165.45544477, 155.00784964,\n",
       "       171.04558668,  61.45763075,  71.66975626, 114.96330486,\n",
       "        51.57808027, 167.57781958, 152.52505798,  62.95827693,\n",
       "       103.49862017, 109.20495627, 175.63844013, 154.60247734,\n",
       "        94.41476124, 210.74244148, 120.25601864,  77.61590087,\n",
       "       187.93503183, 206.49543321, 140.63018684, 105.59463059,\n",
       "       130.704246  , 202.18650868, 171.1330116 , 164.91246096,\n",
       "       124.72637597, 144.81210187, 181.99631481, 199.41234515,\n",
       "       234.21402489, 145.96053305,  79.86349114, 157.36828831,\n",
       "       192.74737754, 208.8980067 , 158.58505486, 206.0226849 ,\n",
       "       107.47978402, 140.93428553,  54.81856678,  55.92807758,\n",
       "       115.00974554,  78.95886675,  81.55731377,  54.3774778 ,\n",
       "       166.25477778])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (9) 모델 학습하기\n",
    "# 정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "# loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "# 입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요.\n",
    "losses = []\n",
    "\n",
    "for i in range(1, 50001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L) # loss값 리스트 추가\n",
    "    if i % 1000 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f54cf0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900.1732878832318"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10) test 데이터에 대한 성능 확인하기\n",
    "# test 데이터에 대한 성능을 확인해주세요.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# !!! 1. 프로젝트 1의 회귀모델 예측정확도가 기준 이상 높게 나왔는가?\tMSE 손실함수값 3000 이하를 달성\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92e53496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], predictions, label=\"pred\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3410d89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n회고\\n1. 실습을 진행하면서 지금까지 배웠던 부분에 대해서 좀 더 이해할 수 있었다.\\n2. 경사 하강법에서 LEARNING_RATE = 0.01 이 부분을 0.0001로 했더니 너무 loss값이 낮아지지 않아서 반복 횟수를 늘리고 LEARNING_RATE를 높였더니\\n수월하게 진행이 되었다.\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "회고\n",
    "1. 실습을 진행하면서 지금까지 배웠던 부분에 대해서 좀 더 이해할 수 있었다.\n",
    "2. 경사 하강법에서 LEARNING_RATE = 0.01 이 부분을 0.0001로 했더니 너무 loss값이 낮아지지 않아서 반복 횟수를 늘리고 LEARNING_RATE를 높였더니\n",
    "수월하게 진행이 되었다.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
